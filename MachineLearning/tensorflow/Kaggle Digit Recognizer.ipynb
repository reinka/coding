{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import input_data\n",
    "from sklearn.preprocessing import normalize, LabelBinarizer\n",
    "\n",
    "\n",
    "# Handle Kaggle input data\n",
    "def dense_to_one_hot(labels_dense, num_classes=10):\n",
    "    \"\"\"Convert class labels from scalars to one-hot vectors.\"\"\"\n",
    "    num_labels = labels_dense.shape[0]\n",
    "    index_offset = np.arange(num_labels) * num_classes\n",
    "    labels_one_hot = np.zeros((num_labels, num_classes))\n",
    "    labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
    "    return labels_one_hot\n",
    "\n",
    "\n",
    "class MnistDataSets(object):\n",
    "    \"\"\"\n",
    "    Convenience class for Training-, Validation- and Testset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_path : str\n",
    "        Path to the train.csv\n",
    "    test_path : str\n",
    "        Path to the test.csv\n",
    "    \"\"\"\n",
    "    def __init__(self, train_path, test_path):\n",
    "        train_df = pd.read_csv(train_path)\n",
    "        y_train = train_df[['label']]\n",
    "        x_train = train_df.ix[:, 1:]\n",
    "        x_train, x_val, y_train, y_val = (x_train.iloc[:30000, :].values,  x_train.iloc[30000:, :].values, \n",
    "                                         y_train[:30000], y_train[30000:])\n",
    "        lb = LabelBinarizer()\n",
    "        lb.fit([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "\n",
    "        self.train = DataSet(x_train, lb.transform(y_train))\n",
    "        self.validation = DataSet(x_val, lb.transform(y_val))\n",
    "\n",
    "        test_images = pd.read_csv(test_path)\n",
    "        self.test = DataSet(test_images)\n",
    "\n",
    "\n",
    "class DataSet(object):\n",
    "    def __init__(self, images, labels=None, fake_data=False):\n",
    "        if fake_data:\n",
    "            self._num_examples = 10000\n",
    "        else:\n",
    "            if labels is not None:\n",
    "                assert images.shape[0] == labels.shape[0], (\n",
    "                    \"images.shape: %s labels.shape: %s\" % (images.shape,\n",
    "                                                           labels.shape))\n",
    "        self._num_examples = images.shape[0]\n",
    "        images = images.astype(np.float32)\n",
    "        images = np.multiply(images, 1.0 / 255.0)\n",
    "        self._images = images\n",
    "        self._labels = labels\n",
    "        self._epochs_completed = 0\n",
    "        self._index_in_epoch = 0\n",
    "\n",
    "    @property\n",
    "    def images(self):\n",
    "        return self._images\n",
    "\n",
    "    @property\n",
    "    def labels(self):\n",
    "        return self._labels\n",
    "\n",
    "    @property\n",
    "    def num_examples(self):\n",
    "        return self._num_examples\n",
    "\n",
    "    @property\n",
    "    def epochs_completed(self):\n",
    "        return self._epochs_completed\n",
    "\n",
    "    def next_batch(self, batch_size):\n",
    "        \"\"\"Return the next `batch_size` examples from this data set.\"\"\"\n",
    "        start = self._index_in_epoch\n",
    "        self._index_in_epoch += batch_size\n",
    "        if self._index_in_epoch > self._num_examples:\n",
    "            # Finished epoch\n",
    "            self._epochs_completed += 1\n",
    "            # Shuffle the data\n",
    "            perm = np.arange(self._num_examples)\n",
    "            np.random.shuffle(perm)\n",
    "            self._images = self._images[perm]\n",
    "            self._labels = self._labels[perm]\n",
    "            # Start next epoch\n",
    "            start = 0\n",
    "            self._index_in_epoch = batch_size\n",
    "            assert batch_size <= self._num_examples\n",
    "        end = self._index_in_epoch\n",
    "        return self._images[start:end], self._labels[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mnist = MnistDataSets(r'/Users/slackoverflow/Downloads/train (1).csv', r'/Users/slackoverflow/Downloads/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P = 0.913\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(\"float\", [None, 784])\n",
    "W = tf.Variable(tf.zeros([784,10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "y = tf.nn.softmax(tf.matmul(x,W) + b)\n",
    "y_ = tf.placeholder(\"float\", [None,10])\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for i in range(1000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "print (\"P =\",sess.run(accuracy, feed_dict={x: mnist.validation.images, y_: mnist.validation.labels}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = sess.run(tf.argmax(y,1),\n",
    "                       feed_dict={x: mnist.test.images})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "predictions = predictions.transpose()\n",
    "data = zip(range(1, len(predictions) + 1), predictions)\n",
    "np.savetxt(\"predictions-%s.csv\" % dt.now().strftime(\"%Y-%m-%d-%H-%M\"),\n",
    "           list(data),\n",
    "           header='ImageId,Label',\n",
    "           comments='',\n",
    "           fmt='%d,%d')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
