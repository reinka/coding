{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "# from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in _notmist.ipynb_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "regularization = 0.01\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "  # Data\n",
    "  X_train = tf.placeholder(dtype=tf.float32, shape=[batch_size, image_size*image_size])\n",
    "  y_train = tf.placeholder(dtype=tf.float32, shape=[batch_size, num_labels])\n",
    "  X_val = tf.constant(valid_dataset)\n",
    "  X_test = tf.constant(test_dataset)\n",
    "  beta = tf.placeholder(dtype=tf.float32)\n",
    "\n",
    "  # Parameters\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal(shape=[image_size*image_size, num_labels]))\n",
    "  bias = tf.Variable(tf.zeros(shape=[num_labels]))\n",
    "  \n",
    "  # Logits and Loss\n",
    "  logits = tf.matmul(X_train, weights) + bias \n",
    "  loss = tf.nn.softmax_cross_entropy_with_logits(logits, y_train)\n",
    "  \n",
    "  # Reguularization\n",
    "  loss += beta * tf.nn.l2_loss(weights)\n",
    "  loss = tf.reduce_mean(loss)\n",
    "  \n",
    "  # Optimizer\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  train_pred = tf.nn.softmax(logits)\n",
    "  val_pred = tf.nn.softmax(tf.matmul(X_val, weights) + bias)\n",
    "  test_pred = tf.nn.softmax(tf.matmul(X_test, weights) + bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 47.936279\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 13.1%\n",
      "Minibatch loss at step 500: 0.866828\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 81.7%\n",
      "Minibatch loss at step 1000: 0.685021\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 81.3%\n",
      "Minibatch loss at step 1500: 0.794337\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 81.0%\n",
      "Minibatch loss at step 2000: 0.811118\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 81.8%\n",
      "Minibatch loss at step 2500: 0.930531\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 81.9%\n",
      "Minibatch loss at step 3000: 0.945361\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 81.7%\n",
      "Test accuracy: 88.8%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {X_train : batch_data, y_train : batch_labels, beta:regularization}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_pred], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        val_pred.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_pred.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Test Accuracy by L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Initialized\n",
      "Initialized\n",
      "Initialized\n",
      "Initialized\n",
      "Initialized\n",
      "Initialized\n",
      "Initialized\n",
      "Initialized\n",
      "Initialized\n",
      "Initialized\n",
      "Initialized\n",
      "Initialized\n",
      "Initialized\n",
      "Initialized\n",
      "Initialized\n",
      "Initialized\n",
      "Initialized\n",
      "Initialized\n",
      "Initialized\n",
      "Initialized\n",
      "Initialized\n",
      "Initialized\n",
      "Initialized\n",
      "Initialized\n",
      "Initialized\n",
      "Initialized\n",
      "Initialized\n",
      "Initialized\n",
      "Initialized\n"
     ]
    }
   ],
   "source": [
    "reg = [pow(10, i) for i in np.arange(-4, -1, 0.1)]\n",
    "accuracies = list()\n",
    "\n",
    "for regularization in reg:\n",
    "  with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "      # Pick an offset within the training data, which has been randomized.\n",
    "      # Note: we could use better randomization across epochs.\n",
    "      offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "      # Generate a minibatch.\n",
    "      batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "      batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "      # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "      # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "      # and the value is the numpy array to feed to it.\n",
    "      feed_dict = {X_train : batch_data, y_train : batch_labels, beta: regularization}\n",
    "      _, l, predictions = session.run(\n",
    "        [optimizer, loss, train_pred], feed_dict=feed_dict)\n",
    "    \n",
    "    accuracies.append(accuracy(val_pred.eval(), valid_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhgAAAF4CAYAAAAWmIDXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x112a832b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.semilogx(reg, accuracies)\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization (1-layer net)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_hidden = 1024\n",
    "beta = 1e-3\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Input data\n",
    "  X_train = tf.placeholder(dtype=tf.float32, \n",
    "                           shape=[batch_size, image_size*image_size])\n",
    "  y_train = tf.placeholder(dtype=tf.float32, \n",
    "                           shape=[batch_size, num_labels])\n",
    "  X_val = tf.constant(valid_dataset)\n",
    "  X_test = tf.constant(test_dataset)\n",
    "  beta = tf.placeholder(tf.float32)\n",
    "  \n",
    "  \n",
    "  # parameters \n",
    "  weights_input = tf.Variable(\n",
    "    tf.truncated_normal(shape=[image_size*image_size, num_hidden]))\n",
    "  bias = tf.Variable(tf.zeros(shape=[num_hidden]))\n",
    "  weights_hidden = tf.Variable(tf.truncated_normal(shape=[num_hidden, num_labels]))\n",
    "  bias2 = tf.Variable(tf.zeros(shape=[num_labels]))\n",
    "  \n",
    "  # Logits, ReLU and Loss\n",
    "  logits_input = tf.matmul(X_train, weights_input) + bias\n",
    "  relu_input = tf.nn.relu(logits_input)\n",
    "  logits_hidden = tf.matmul(relu_input, weights_hidden) + bias2\n",
    "  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits_hidden, y_train))\n",
    "  \n",
    "  # Add regularization\n",
    "  loss += beta * (tf.nn.l2_loss(weights_input) + tf.nn.l2_loss(weights_hidden))\n",
    "  #loss = tf.reduce_mean(loss)\n",
    "  \n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predict\n",
    "  train_pred = tf.nn.softmax(logits_hidden)\n",
    "  val_layer1 = tf.nn.relu(tf.matmul(X_val, weights_input) + bias)\n",
    "  val_pred = tf.nn.softmax(tf.matmul(val_layer1, weights_hidden) + bias2)\n",
    "  test_layer1 = tf.nn.relu(tf.matmul(X_test, weights_input) + bias)\n",
    "  test_pred = tf.nn.softmax(tf.matmul(test_layer1, weights_hidden) + bias2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 638.656250\n",
      "Minibatch accuracy: 7.8%\n",
      "Validation accuracy: 33.0%\n",
      "Minibatch loss at step 500: 194.389755\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 81.2%\n",
      "Minibatch loss at step 1000: 113.787880\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 81.6%\n",
      "Minibatch loss at step 1500: 68.882332\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 81.8%\n",
      "Minibatch loss at step 2000: 41.441406\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 85.1%\n",
      "Minibatch loss at step 2500: 25.332434\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 85.9%\n",
      "Minibatch loss at step 3000: 15.577061\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 86.7%\n",
      "Test accuracy: 93.1%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {X_train : batch_data, y_train : batch_labels, beta : 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_pred], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        val_pred.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_pred.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 662.872803\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 33.5%\n",
      "Minibatch loss at step 5: 354.853088\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 64.6%\n",
      "Minibatch loss at step 10: 311.934479\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.2%\n",
      "Minibatch loss at step 15: 310.378815\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.2%\n",
      "Minibatch loss at step 20: 308.828796\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.2%\n",
      "Minibatch loss at step 25: 307.287506\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.2%\n",
      "Minibatch loss at step 30: 305.756470\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.2%\n",
      "Minibatch loss at step 35: 304.228821\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.2%\n",
      "Minibatch loss at step 40: 302.712494\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.2%\n",
      "Minibatch loss at step 45: 301.202484\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.2%\n",
      "Test accuracy: 71.8%\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "num_batches = 3\n",
    "num_steps = 50\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    #offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    offset = step % num_batches\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {X_train : batch_data, y_train : batch_labels, beta : 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_pred], feed_dict=feed_dict)\n",
    "    if (step % 5 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        val_pred.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_pred.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 774.546875\n",
      "Minibatch accuracy: 11.7%\n",
      "Validation accuracy: 26.3%\n",
      "Minibatch loss at step 5: 374.047516\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 61.9%\n",
      "Minibatch loss at step 10: 316.102295\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 67.1%\n",
      "Minibatch loss at step 15: 311.028229\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 68.3%\n",
      "Minibatch loss at step 20: 309.446747\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.9%\n",
      "Minibatch loss at step 25: 308.921173\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 68.6%\n",
      "Minibatch loss at step 30: 307.397125\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 68.2%\n",
      "Minibatch loss at step 35: 305.901306\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 68.7%\n",
      "Minibatch loss at step 40: 303.539581\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 68.5%\n",
      "Minibatch loss at step 45: 301.870453\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.5%\n",
      "Test accuracy: 74.5%\n"
     ]
    }
   ],
   "source": [
    "num_hidden = 1024\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Input data\n",
    "  X_train = tf.placeholder(dtype=tf.float32, \n",
    "                           shape=[batch_size, image_size*image_size])\n",
    "  y_train = tf.placeholder(dtype=tf.float32, \n",
    "                           shape=[batch_size, num_labels])\n",
    "  X_val = tf.constant(valid_dataset)\n",
    "  X_test = tf.constant(test_dataset)\n",
    "  beta = tf.placeholder(tf.float32)\n",
    "  \n",
    "  \n",
    "  # parameters \n",
    "  weights_input = tf.Variable(\n",
    "    tf.truncated_normal(shape=[image_size*image_size, num_hidden]))\n",
    "  bias = tf.Variable(tf.zeros(shape=[num_hidden]))\n",
    "  weights_hidden = tf.Variable(tf.truncated_normal(shape=[num_hidden, num_labels]))\n",
    "  bias2 = tf.Variable(tf.zeros(shape=[num_labels]))\n",
    "  \n",
    "  # Logits, ReLU and Loss\n",
    "  logits_input = tf.matmul(X_train, weights_input) + bias\n",
    "  relu_input = tf.nn.relu(logits_input)\n",
    "  relu_input = tf.nn.dropout(relu_input, 0.5)\n",
    "  logits_hidden = tf.matmul(relu_input, weights_hidden) + bias2\n",
    "  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits_hidden, y_train))\n",
    "  \n",
    "  # Add regularization\n",
    "  loss += beta * (tf.nn.l2_loss(weights_input) + tf.nn.l2_loss(weights_hidden))\n",
    "#  loss = tf.reduce_mean(loss)\n",
    "  \n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predict\n",
    "  train_pred = tf.nn.softmax(logits_hidden)\n",
    "  val_layer1 = tf.nn.relu(tf.matmul(X_val, weights_input) + bias)\n",
    "  val_pred = tf.nn.softmax(tf.matmul(val_layer1, weights_hidden) + bias2)\n",
    "  test_layer1 = tf.nn.relu(tf.matmul(X_test, weights_input) + bias)\n",
    "  test_pred = tf.nn.softmax(tf.matmul(test_layer1, weights_hidden) + bias2)\n",
    "  \n",
    "batch_size = 128\n",
    "num_batches = 3\n",
    "num_steps = 50\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    #offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    offset = step % num_batches\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {X_train : batch_data, y_train : batch_labels, beta : 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_pred], feed_dict=feed_dict)\n",
    "    if (step % 5 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        val_pred.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_pred.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_hidden1 = 1024\n",
    "batch_size = 128\n",
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "  X_train = tf.placeholder(dtype=tf.float32, \n",
    "                           shape=[batch_size, image_size**2])\n",
    "  y_train = tf.placeholder(dtype=tf.float32, \n",
    "                          shape=[batch_size, num_labels])\n",
    "  X_val = tf.constant(valid_dataset)\n",
    "  X_test = tf.constant(test_dataset)\n",
    "  beta = tf.placeholder(dtype=tf.float32)\n",
    "  \n",
    "  \n",
    "  # Parameters\n",
    "  # Input layer\n",
    "  weights_input = tf.Variable(\n",
    "    tf.truncated_normal(shape=[image_size**2, num_hidden1]))\n",
    "  bias_to_hidden1 = tf.Variable(tf.zeros(shape=[num_hidden1]))\n",
    "  \n",
    "  # 1st hidden layer\n",
    "  weights_hidden1 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden1, num_labels]))\n",
    "  bias_to_output = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Compute Logits, ReLU, Dropout\n",
    "  logits_input = tf.nn.relu(tf.matmul(X_train, weights_input) + bias_to_hidden1)\n",
    "  # Dropout\n",
    "  logits_input = tf.nn.dropout(logits_input, 0.5)\n",
    "  \n",
    "  final_logits = tf.matmul(logits_input, weights_hidden1) + bias_to_output\n",
    "  \n",
    "  # Comput Cross entropy\n",
    "  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(final_logits, y_train))\n",
    "  \n",
    "  # Add Regularization\n",
    "  loss += beta * ( tf.nn.l2_loss(weights_input) + \n",
    "                  tf.nn.l2_loss(weights_hidden1))\n",
    "  \n",
    "  # Optimizer\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.25).minimize(loss)\n",
    "  \n",
    "  # Predict\n",
    "  train_pred = tf.nn.softmax(final_logits)\n",
    "  val_layer1 = tf.nn.relu(tf.matmul(X_val, weights_input) + bias_to_hidden1)\n",
    "  val_pred = tf.nn.softmax(tf.matmul(val_layer1, weights_hidden1) + bias_to_output)\n",
    "  test_layer1 = tf.nn.relu(tf.matmul(X_test, weights_input) + bias_to_hidden1)\n",
    "  test_pred = tf.nn.softmax(tf.matmul(test_layer1, weights_hidden1) + bias_to_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 767.271484\n",
      "Minibatch accuracy: 13.3%\n",
      "Validation accuracy: 32.6%\n",
      "Minibatch loss at step 500: 260.529236\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy: 82.5%\n",
      "Minibatch loss at step 1000: 197.350357\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 82.3%\n",
      "Minibatch loss at step 1500: 147.661438\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 82.8%\n",
      "Minibatch loss at step 2000: 118.017479\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 82.7%\n",
      "Minibatch loss at step 2500: 90.051178\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 83.2%\n",
      "Minibatch loss at step 3000: 68.809753\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 83.9%\n",
      "Test accuracy: 90.1%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick offset \n",
    "    items = np.random.randint(0, train_dataset.shape[0], batch_size)\n",
    "    # Generate minibatch\n",
    "    batch_data = train_dataset[items,:]\n",
    "    batch_labels = train_labels[items,:]\n",
    "    # Prepare dict telling the session where to feed the minibatch.\n",
    "    # Key of dict is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed it\n",
    "    feed_dict = {X_train : batch_data, y_train : batch_labels, beta: 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_pred], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print('Minibatch loss at step %d: %f'% (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        val_pred.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_pred.eval(), test_labels))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 4.296863\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 25.9%\n",
      "Minibatch loss at step 500: 1.607152\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 85.2%\n",
      "Minibatch loss at step 1000: 1.180095\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 85.5%\n",
      "Minibatch loss at step 1500: 0.999677\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 2000: 0.868450\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 87.8%\n",
      "Minibatch loss at step 2500: 0.889679\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 87.9%\n",
      "Test accuracy: 94.0%\n"
     ]
    }
   ],
   "source": [
    "num_hidden1 = 1024\n",
    "num_hidden2 = 1024\n",
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "  X_train = tf.placeholder(dtype=tf.float32, \n",
    "                           shape=[batch_size, image_size**2])\n",
    "  y_train = tf.placeholder(dtype=tf.float32, \n",
    "                          shape=[batch_size, num_labels])\n",
    "  X_val = tf.constant(valid_dataset)\n",
    "  X_test = tf.constant(test_dataset)\n",
    "  beta = tf.placeholder(dtype=tf.float32)\n",
    "  global_step = tf.Variable(0) # count the number of steps taken.\n",
    "  \n",
    "  \n",
    "  # Parameters\n",
    "  # Input layer\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal(shape=[image_size*image_size, num_hidden1],stddev=np.sqrt(2.0 / (image_size*image_size))))\n",
    "  bias1 = tf.Variable(tf.zeros(shape=[num_hidden1]))\n",
    "  \n",
    "  # 1st hidden layer\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden1, num_hidden2], stddev=np.sqrt(2.0 / num_hidden1)))\n",
    "  bias2 = tf.Variable(tf.zeros([num_hidden2]))\n",
    "  \n",
    "  # 2nd hidden layer\n",
    "  weights3 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden2, num_labels],stddev=np.sqrt(2.0 / num_hidden2)))\n",
    "  bias3 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Compute Logits, ReLU, Dropout\n",
    "  lay1 = tf.nn.relu(tf.matmul(X_train, weights1) + bias1)\n",
    "  # Dropout\n",
    "  lay1 = tf.nn.dropout(lay1, 0.5)\n",
    "  \n",
    "  lay2 = tf.nn.relu(tf.matmul(lay1, weights2) + bias2)\n",
    "  # Dropout\n",
    "  lay2 = tf.nn.dropout(lay2, 0.5)\n",
    "  \n",
    "  logits = tf.matmul(lay2, weights3) + bias3\n",
    "  \n",
    "  # Comput Cross entropy\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, y_train)) + beta * (tf.nn.l2_loss(weights1) + \n",
    "                  tf.nn.l2_loss(weights2) + \n",
    "                  tf.nn.l2_loss(weights3))\n",
    "  \n",
    "  # Optimizer\n",
    "  learning_rate = tf.train.exponential_decay(0.5, global_step, 1000, 0.65, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predict\n",
    "  train_pred = tf.nn.softmax(logits)\n",
    "  \n",
    "  val_layer1 = tf.nn.relu(tf.matmul(X_val, weights1) + bias1)\n",
    "  val_layer2 = tf.nn.relu(tf.matmul(val_layer1, weights2) + bias2)\n",
    "  val_pred = tf.nn.softmax(tf.matmul(val_layer2, weights3) + bias3)\n",
    "  \n",
    "  test_layer1 = tf.nn.relu(tf.matmul(X_test, weights1) + bias1)\n",
    "  test_layer2 = tf.nn.relu(tf.matmul(test_layer1, weights2) + bias2)\n",
    "  test_pred = tf.nn.softmax(tf.matmul(test_layer2, weights3) + bias3)\n",
    "  \n",
    "num_steps = 3000\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick offset \n",
    "    #items = np.random.randint(0, train_dataset.shape[0], batch_size)\n",
    "    # Generate minibatch\n",
    "    #batch_data = train_dataset[items,:]\n",
    "    #batch_labels = train_labels[items,:]\n",
    "    \n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare dict telling the session where to feed the minibatch.\n",
    "    # Key of dict is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed it\n",
    "    feed_dict = {X_train : batch_data, y_train : batch_labels, beta: 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_pred], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      #print(items)\n",
    "      print('Minibatch loss at step %d: %f'% (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        val_pred.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_pred.eval(), test_labels))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO:\n",
    "    Research whether it makes sense to combine dropout with L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3.859939\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 17.9%\n",
      "Minibatch loss at step 500: 1.422711\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 84.7%\n",
      "Minibatch loss at step 1000: 1.157841\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 86.0%\n",
      "Minibatch loss at step 1500: 0.796755\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 2000: 0.813236\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch loss at step 2500: 0.843390\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 87.6%\n",
      "Test accuracy: 93.7%\n"
     ]
    }
   ],
   "source": [
    "num_hidden1 = 1024\n",
    "num_hidden2 = 512\n",
    "batch_size = 128\n",
    "starter_learning_rate = 0.5\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "  X_train = tf.placeholder(dtype=tf.float32, \n",
    "                           shape=[batch_size, image_size**2])\n",
    "  y_train = tf.placeholder(dtype=tf.float32, \n",
    "                          shape=[batch_size, num_labels])\n",
    "  X_val = tf.constant(valid_dataset)\n",
    "  X_test = tf.constant(test_dataset)\n",
    "  beta = tf.placeholder(dtype=tf.float32)\n",
    "  global_step = tf.Variable(0) # count the number of steps taken.\n",
    "  \n",
    "  \n",
    "  # Parameters\n",
    "  # Input layer\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal(shape=[image_size*image_size, num_hidden1],stddev=np.sqrt(2.0 / (image_size*image_size))))\n",
    "  bias1 = tf.Variable(tf.zeros(shape=[num_hidden1]))\n",
    "  \n",
    "  # 1st hidden layer\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden1, num_hidden2], stddev=np.sqrt(2.0 / num_hidden1)))\n",
    "  bias2 = tf.Variable(tf.zeros([num_hidden2]))\n",
    "  \n",
    "  # 2nd hidden layer\n",
    "  weights3 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden2, num_labels],stddev=np.sqrt(2.0 / num_hidden2)))\n",
    "  bias3 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Compute Logits, ReLU, Dropout\n",
    "  lay1 = tf.nn.relu(tf.matmul(X_train, weights1) + bias1)\n",
    "  # Dropout\n",
    "  lay1 = tf.nn.dropout(lay1, 0.5)\n",
    "  \n",
    "  lay2 = tf.nn.relu(tf.matmul(lay1, weights2) + bias2)\n",
    "  # Dropout\n",
    "  lay2 = tf.nn.dropout(lay2, 0.5)\n",
    "  \n",
    "  logits = tf.matmul(lay2, weights3) + bias3\n",
    "  \n",
    "  # Comput Cross entropy\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, y_train)) + beta * (tf.nn.l2_loss(weights1) + \n",
    "                  tf.nn.l2_loss(weights2) + \n",
    "                  tf.nn.l2_loss(weights3))\n",
    "  \n",
    "  # Optimizer\n",
    "  learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, 1000, 0.65, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predict\n",
    "  train_pred = tf.nn.softmax(logits)\n",
    "  \n",
    "  val_layer1 = tf.nn.relu(tf.matmul(X_val, weights1) + bias1)\n",
    "  val_layer2 = tf.nn.relu(tf.matmul(val_layer1, weights2) + bias2)\n",
    "  val_pred = tf.nn.softmax(tf.matmul(val_layer2, weights3) + bias3)\n",
    "  \n",
    "  test_layer1 = tf.nn.relu(tf.matmul(X_test, weights1) + bias1)\n",
    "  test_layer2 = tf.nn.relu(tf.matmul(test_layer1, weights2) + bias2)\n",
    "  test_pred = tf.nn.softmax(tf.matmul(test_layer2, weights3) + bias3)\n",
    "  \n",
    "num_steps = 3000\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick offset \n",
    "    items = np.random.randint(0, train_dataset.shape[0], batch_size)\n",
    "    # Generate minibatch\n",
    "    batch_data = train_dataset[items,:]\n",
    "    batch_labels = train_labels[items,:]\n",
    "    \n",
    "    #offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    #batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    #batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare dict telling the session where to feed the minibatch.\n",
    "    # Key of dict is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed it\n",
    "    feed_dict = {X_train : batch_data, y_train : batch_labels, beta: 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_pred], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      #print(items)\n",
    "      print('Minibatch loss at step %d: %f'% (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        val_pred.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_pred.eval(), test_labels))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
