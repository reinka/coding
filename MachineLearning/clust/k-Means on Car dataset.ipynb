{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of k-Means\n",
    "    using the car dataset from the UCI Machine Learning Repository. \n",
    "    (source: https://archive.ics.uci.edu/ml/datasets/Car+Evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>small</td>\n",
       "      <td>low</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>small</td>\n",
       "      <td>med</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>small</td>\n",
       "      <td>high</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>med</td>\n",
       "      <td>low</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>med</td>\n",
       "      <td>med</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0    0      1      2    3    4      5     6      7\n",
       "0           0  0.0  vhigh  vhigh  2.0  2.0  small   low  unacc\n",
       "1           1  1.0  vhigh  vhigh  2.0  2.0  small   med  unacc\n",
       "2           2  2.0  vhigh  vhigh  2.0  2.0  small  high  unacc\n",
       "3           3  3.0  vhigh  vhigh  2.0  2.0    med   low  unacc\n",
       "4           4  4.0  vhigh  vhigh  2.0  2.0    med   med  unacc"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "path = '/Users/slackoverflow/Dropbox/DKE/Machine Learning/Decision Trees/ID3/data.csv'\n",
    "data = pd.read_csv(path)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>small</td>\n",
       "      <td>low</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>small</td>\n",
       "      <td>med</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>small</td>\n",
       "      <td>high</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>med</td>\n",
       "      <td>low</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>med</td>\n",
       "      <td>med</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       1      2    3    4      5     6      7\n",
       "0  vhigh  vhigh  2.0  2.0  small   low  unacc\n",
       "1  vhigh  vhigh  2.0  2.0  small   med  unacc\n",
       "2  vhigh  vhigh  2.0  2.0  small  high  unacc\n",
       "3  vhigh  vhigh  2.0  2.0    med   low  unacc\n",
       "4  vhigh  vhigh  2.0  2.0    med   med  unacc"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del data['Unnamed: 0']\n",
    "del data['0']\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Transform non-numerical labels to numerical ones via sklearn preprocessor\n",
    "    One could do this also by hand but since this part isn't important for understanding\n",
    "    the k-Means algorithm, I've decided to use a more convenient approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   1  2  3  4  5  6  7\n",
       "0  3  3  0  0  2  1  2\n",
       "1  3  3  0  0  2  2  2\n",
       "2  3  3  0  0  2  0  2\n",
       "3  3  3  0  0  1  1  2\n",
       "4  3  3  0  0  1  2  2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_data = data.apply(LabelEncoder().fit_transform)\n",
    "transformed_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the distribution of the outcome labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of class labels in train set:\n",
      "2    1210\n",
      "0     384\n",
      "1      69\n",
      "3      65\n",
      "Name: 7, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAi4AAAGHCAYAAACXsdlkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwUWSJHXG/w8BliO3SRvYsAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1137a1a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "labels = transformed_data.values[:,-1]\n",
    "\n",
    "print(\"Distribution of class labels in train set:\")\n",
    "print(transformed_data['7'].value_counts())\n",
    "\n",
    "plt.hist(labels)\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Histogram of class labels\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Majority of classes belong to class 2. So we should expect most clusters to be classified as 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class k_means:\n",
    "    def __init__(self, data, k, initialization=None):\n",
    "        self.data = data\n",
    "        self.dim = data.shape[1] - 1\n",
    "        self.x = self.data\n",
    "        self.y = data[:,-1] # ignore for this assignment\n",
    "        self.k = k\n",
    "        self.start = np.vstack(initialization) if initialization else None\n",
    "        self.centroids_history = list()\n",
    "        self.assigned_cluster = None\n",
    "        self.cluster_class = None\n",
    "        self.prediction = None\n",
    "        \n",
    "    def initialize(self):\n",
    "        '''\n",
    "        In case no starting clusters are specified by the user, \n",
    "        k distinct points from the dataset are choosen \n",
    "        randomly as cluster centroids.\n",
    "        '''\n",
    "        \n",
    "        if type(self.start) is type(None):\n",
    "            from random import randint\n",
    "            rand = []\n",
    "            length = len(self.x)\n",
    "            for n in range(self.k):\n",
    "                index = randint(0, length - 1)\n",
    "                \n",
    "                # make sure we haven't already\n",
    "                # picked this point\n",
    "                if index in rand:\n",
    "                    while(index in rand):\n",
    "                        index = randint(0,length-1)\n",
    "                \n",
    "                rand.append(index)\n",
    "                \n",
    "            self.start = np.vstack([self.data[k] for k in rand])\n",
    "\n",
    "        self.centroids_history.append(self.start)\n",
    "            \n",
    "    \n",
    "    def kmeans(self, max_iter = 500, verbose = False):\n",
    "        if self.k > len(self.x):\n",
    "            return \"Error: k > #(data points). Goodbye.\"\n",
    "        # initialize first\n",
    "        self.initialize()\n",
    "        \n",
    "        # remember previous centroid of each iteration\n",
    "        # will be needed to terminate the algorithm \n",
    "        old_centroids = self.start\n",
    "        new_centroids = self.mapping(old_centroids, verbose)\n",
    "        if verbose:\n",
    "            print(\"After first mapping, new centroids:\")\n",
    "            print(new_centroids)\n",
    "        if(not np.array_equal(new_centroids, old_centroids)):\n",
    "            equal = False\n",
    "            step = 0\n",
    "            \n",
    "            while not equal and step < max_iter:\n",
    "                old_centroids = new_centroids.copy()\n",
    "                new_centroids = self.mapping(old_centroids,verbose)\n",
    "                \n",
    "                # stop when there is no more change\n",
    "                equal = np.array_equal(old_centroids,new_centroids)\n",
    "                step += 1\n",
    "        \n",
    "        # pop last result cuz it's been added twice\n",
    "        # (during the last two iterations of while loop)\n",
    "        self.centroids_history.pop(-1)\n",
    "\n",
    "              \n",
    "    def mapping(self, old_centroids,verbose):\n",
    "        '''\n",
    "        Maps each data point to each nearest cluster\n",
    "        and recomputes the cluster centroids afterwards.\n",
    "        '''\n",
    "        # Create a list of lists to store \n",
    "        # data points which were \n",
    "        # assigned to a specific centroid.\n",
    "        # Each list represents the data points \n",
    "        # of a specific centroid\n",
    "        assignment = [list() for n in range(self.k)]\n",
    "        labels = [list() for n in range(self.k)]\n",
    "        \n",
    "        # create var to store new centroids later \n",
    "        new_centroids = old_centroids.copy()\n",
    "\n",
    "        # for each data point\n",
    "        for data_point in self.data:\n",
    "            dist = []\n",
    "\n",
    "            # compute distance to each centroid\n",
    "            for centroid in new_centroids:\n",
    "                # append distance to a list\n",
    "                dist.append(np.linalg.norm(centroid - data_point))\n",
    "\n",
    "            # assign each data point to its nearest centroid\n",
    "            c = np.argmin(dist)\n",
    "            assignment[c].append(data_point)\n",
    "            #labels[c] \n",
    "            \n",
    "        if verbose:\n",
    "            print(\"Computing new centroids:\")\n",
    "            \n",
    "        # Compute new centroids \n",
    "        result=[]\n",
    "        for n in range(self.k):\n",
    "            result.append(np.mean(assignment[n], axis=0))\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Retruning newly computed centroids:\")\n",
    "            print(result)\n",
    "        \n",
    "        # save result for plotting \n",
    "        self.centroids_history.append(np.vstack(result))\n",
    "        self.assigned_cluster = assignment\n",
    "        return result\n",
    "    \n",
    "    def get_labels(self):\n",
    "        '''\n",
    "        Returns a list that represents \n",
    "        which cluster each data point was assigned to.\n",
    "        '''\n",
    "        # hashmap, where the `key` represents the data point\n",
    "        # and the `value` of that key represents the cluster \n",
    "        # the given data point was assigned to\n",
    "        labels = dict() \n",
    "        cluster = 1\n",
    "        for centroid in self.assigned_cluster:\n",
    "            for data_point in centroid:\n",
    "                labels['%s' % data_point] = cluster\n",
    "            cluster += 1\n",
    "\n",
    "                \n",
    "        return [labels[str(data_point)] for data_point in self.data]\n",
    "      \n",
    "    def classify_clusters(self, verbose=False):\n",
    "      '''\n",
    "      Classify all objects of a cluster with the same class.\n",
    "      This class is the majority class of the training examples of that cluster\n",
    "      '''\n",
    "      \n",
    "      class_by_cluster = dict()\n",
    "      for row,cluster in enumerate(self.get_labels()):\n",
    "        if cluster not in class_by_cluster:\n",
    "          class_by_cluster[cluster] = []\n",
    "  \n",
    "        class_by_cluster[cluster] += [transformed_data.values[row, -1]]\n",
    "    \n",
    "      if verbose:\n",
    "        print(class_by_cluster)\n",
    "        \n",
    "      # Now we have a dictionary, where each `key` represents a cluster\n",
    "      # and its `value` is a list, that contains all the classes that were \n",
    "      # assigned to that cluster\n",
    "      \n",
    "      # Determine majority class. In statistics this is called the `mode`\n",
    "      from scipy import stats\n",
    "      \n",
    "      for key,val in class_by_cluster.items():\n",
    "        class_by_cluster[key] = stats.mode(val)[0] # or np.argmax(np.bincount(val))\n",
    "      \n",
    "      self.cluster_class = class_by_cluster\n",
    "      \n",
    "      return class_by_cluster\n",
    "    \n",
    "    def predict(self):\n",
    "      '''Predict classes based on how the\n",
    "      clusters have been classified. Runtime \n",
    "      is extremly shitty with O(n^3).\n",
    "      Feel free to improve.'''\n",
    "      \n",
    "      # To compute the accuracy, we first have to \n",
    "      # extract the rows of the data set \n",
    "      # that were assigned to each cluster. \n",
    "      \n",
    "      # list of list to store the rows of each cluster\n",
    "      rows = [list() for n in range(k)]\n",
    "      \n",
    "      # finally grab the row indices\n",
    "      for cluster in range(self.k): \n",
    "        for clust_item in self.assigned_cluster[cluster]:\n",
    "          index = 0\n",
    "          for item in self.data:\n",
    "            if np.array_equal(clust_item, item):\n",
    "              rows[cluster].append(index)\n",
    "              \n",
    "            index += 1\n",
    "            \n",
    "      # next construct a vector of length 'number of rows'\n",
    "      n_rows = self.data.shape[0] \n",
    "      pred = np.full((n_rows,), 0, dtype=np.int16)\n",
    "\n",
    "      # Replace the data points by the actual class label\n",
    "      # of each cluster. We have the clusters saved in \n",
    "      # ascending order in `row`, where each cluster \n",
    "      # contains a list with the according row indices\n",
    "      # So indexng into row[0] should return all indices\n",
    "      # of the dataset that are assigned to cluster 1.\n",
    "      for cluster in range(self.k):\n",
    "        pred[rows[cluster]] = self.cluster_class[cluster+1][0]\n",
    "      \n",
    "      self.prediction = pred\n",
    "      # Output\n",
    "      return self.prediction\n",
    "    \n",
    "    def accuracy(self, labels):\n",
    "      '''Computes prediction accuracy'''\n",
    "  \n",
    "      return (100.0 * np.sum(self.prediction == labels) \n",
    "              / self.prediction.shape[0])\n",
    "      \n",
    "    \n",
    "    def visualize(self):\n",
    "      '''Plot cluster centroids in case the data is 2 dimensional'''\n",
    "      \n",
    "      if self.dim > 2: \n",
    "        return print(\"Can't visualize data with more than 2 dimensions.\")\n",
    "        \n",
    "        x, y = self.x, self.y\n",
    "        min_x = np.amin(self.start,axis=0)[0]\n",
    "        max_x = np.amax(self.start, axis=0)[0]\n",
    "\n",
    "        min_y = np.amin(self.start, axis=0)[1]\n",
    "        max_y = np.amax(self.start, axis=0)[1]\n",
    "        \n",
    "        min_x = min(min_x, min(x))\n",
    "        max_x = max(max_x, max(x))\n",
    "        \n",
    "        \n",
    "        min_y = min(min_y, min(y))\n",
    "        max_y = max(max_y, max(y))\n",
    "        \n",
    "        colors = ['b','g','r','c','m','y','k','w']\n",
    "        plt.plot(x,y, 'o', markersize=6)\n",
    "        plt.ylim(min_y-1,max_y+1)\n",
    "        plt.xlim(min_x-1, max_x+1)\n",
    "        \n",
    "        size = 7\n",
    "        for centroids in self.centroids_history:\n",
    "            size += size*0.25\n",
    "            for i in range(self.k):\n",
    "                plt.plot(centroids[i][0], centroids[i][1], '*', \n",
    "                         markersize=size, color=\"{}\".format(colors[i]))\n",
    "        \n",
    "        plt.xlabel('x')\n",
    "        plt.ylabel('y')\n",
    "        plt.title('Scatterplot of Dataset')\n",
    "\n",
    "        plt.show()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specify k: 4\n",
      "Initializing Centroids\n",
      "Done.\n",
      "Running k-Means with k = 4\n",
      "Done.\n",
      "Result of cluster classification:\n",
      "{1: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 0, 2, 0, 0, 2, 2, 2, 2, 2, 0, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 0, 2, 0, 0, 2, 2, 0, 2, 0, 0, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 0, 2, 0, 0, 2, 2, 2, 2, 2, 0, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 0, 2, 0, 0, 2, 2, 0, 2, 0, 0, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 0, 2, 0, 0, 2, 2, 2, 2, 2, 0, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 0, 2, 0, 0, 2, 2, 2, 2, 2, 0, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 0, 0, 2, 0, 3, 2, 2, 2, 2, 0, 0, 2, 0, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 0, 0, 2, 0, 3, 2, 0, 0, 2, 0, 3, 2, 0, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 1, 2, 0, 1, 2, 1, 3, 2, 2, 2, 2, 0, 1, 2, 1, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 1, 2, 0, 1, 2, 1, 3, 2, 0, 1, 2, 1, 3, 2, 1, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 1, 2, 0, 1, 2, 1, 3, 2, 2, 2, 2, 0, 1, 2, 1, 3], 2: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 0, 2, 0, 0, 2, 2, 0, 2, 0, 0, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 0, 2, 0, 0, 2, 2, 0, 2, 0, 0, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 0, 2, 0, 0, 2, 2, 0, 2, 0, 0, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 0, 2, 0, 0, 2, 2, 0, 2, 0, 0, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 0, 2, 0, 0, 2, 2, 0, 2, 0, 0, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 0, 2, 0, 0, 2, 2, 0, 2, 0, 0, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 0, 2, 0, 0, 2, 2, 0, 2, 0, 0, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 0, 3, 2, 0, 3, 2, 0, 0, 2, 0, 3, 2, 0, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 0, 3, 2, 0, 3, 2, 0, 0, 2, 0, 3, 2, 0, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 1, 2, 1, 3, 2, 1, 3, 2, 0, 1, 2, 1, 3, 2, 1, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 1, 2, 1, 3, 2, 1, 3, 2, 0, 1, 2, 1, 3, 2, 1, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 1, 2, 1, 3, 2, 1, 3, 2, 0, 1, 2, 1, 3, 2, 1, 3], 3: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 0, 2, 0, 0, 2, 2, 2, 2, 2, 0, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 0, 2, 0, 0, 2, 2, 0, 2, 0, 0, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 0, 2, 0, 0, 2, 2, 0, 2, 0, 0, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 0, 2, 0, 0, 2, 2, 0, 2, 0, 0, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 0, 2, 0, 0, 2, 2, 2, 2, 2, 0, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 0, 2, 0, 0, 2, 2, 0, 2, 0, 0, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 0, 2, 0, 0, 2, 2, 0, 2, 0, 0, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 0, 2, 0, 0, 2, 2, 0, 2, 0, 0, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 0, 2, 0, 0, 2, 2, 0, 2, 0, 0, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 0, 0, 2, 0, 3, 2, 2, 2, 2, 0, 0, 2, 0, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 0, 0, 2, 0, 3, 2, 0, 0, 2, 0, 3, 2, 0, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 0, 3, 2, 0, 3, 2, 0, 0, 2, 0, 3, 2, 0, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 0, 3, 2, 0, 3, 2, 0, 0, 2, 0, 3, 2, 0, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 1, 2, 0, 1, 2, 1, 3, 2, 2, 2, 2, 0, 1, 2, 1, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 1, 2, 0, 1, 2, 1, 3, 2, 0, 1, 2, 1, 3, 2, 1, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 1, 2, 1, 3, 2, 1, 3, 2, 0, 1, 2, 1, 3, 2, 1, 3], 4: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 0, 2, 0, 0, 2, 2, 2, 2, 2, 0, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 0, 2, 0, 0, 2, 2, 0, 2, 0, 0, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 0, 2, 0, 0, 2, 2, 0, 2, 0, 0, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 0, 2, 0, 0, 2, 2, 0, 2, 0, 0, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 0, 2, 0, 0, 2, 2, 0, 2, 0, 0, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 0, 2, 0, 0, 2, 2, 0, 2, 0, 0, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 0, 2, 0, 0, 2, 2, 2, 2, 2, 0, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 0, 2, 0, 0, 2, 2, 0, 2, 0, 0, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 0, 2, 0, 0, 2, 2, 0, 2, 0, 0, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 0, 2, 0, 0, 2, 2, 0, 2, 0, 0, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 1, 2, 0, 1, 2, 1, 3, 2, 0, 1, 2, 1, 3, 2, 1, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 1, 2, 1, 3, 2, 1, 3, 2, 0, 1, 2, 1, 3, 2, 1, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 1, 2, 1, 3, 2, 1, 3, 2, 0, 1, 2, 1, 3, 2, 1, 3]}\n",
      "Cluster 1 assigned to class 2\n",
      "Cluster 2 assigned to class 2\n",
      "Cluster 3 assigned to class 2\n",
      "Cluster 4 assigned to class 2\n"
     ]
    }
   ],
   "source": [
    "X = transformed_data.values\n",
    "k = int(input(\"Specify k: \"))\n",
    "\n",
    "# initialize new starting points\n",
    "print('Initializing Centroids')\n",
    "test = k_means(X[:,:-1], k=k)\n",
    "print(\"Done.\")\n",
    "\n",
    "# run k-Means\n",
    "print(\"Running k-Means with k = %d\" %k)\n",
    "test.kmeans()\n",
    "print(\"Done.\")\n",
    "print(\"Result of cluster classification:\")\n",
    "for key, val in test.classify_clusters(verbose=True).items():\n",
    "  print(\"Cluster %d assigned to class %d\" % (key, val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####    All but Clusters are assigned to class 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case this step could have been handled easier by just taking the relative frequency of labels that belong to class 2. Since generally one can not expect to have all clusters assigned to the same class, I still implemented a predict() method that differentiates between each cluster's assigned class and predicts accordingly instead of just outputing a vecotr consisting of 2s. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels. This is gonna take a while (O(n^3) runtime...))\n",
      "...\n",
      "Done.  [2 2 2 ..., 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicting labels. This is gonna take a while (O(n^3) runtime...))\")\n",
    "print(\"...\")\n",
    "print(\"Done. \", test.predict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy: 70.02%\n"
     ]
    }
   ],
   "source": [
    "# labels from the beginning of this notebook\n",
    "print(\"Prediction accuracy: %.2f%%\" % test.accuracy(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The result yields an accuracy of 70.02% which isn't better than a baseline classifier that classifies each input according to the most frequent class of the training set. Since the relative frequency of class 2 is 1210/1728 = 70.02% we're not doing any better here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Append new column `cluster` to the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1718</th>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>5-more</td>\n",
       "      <td>4.0</td>\n",
       "      <td>big</td>\n",
       "      <td>high</td>\n",
       "      <td>vgood</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1719</th>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>5-more</td>\n",
       "      <td>more</td>\n",
       "      <td>small</td>\n",
       "      <td>low</td>\n",
       "      <td>unacc</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1720</th>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>5-more</td>\n",
       "      <td>more</td>\n",
       "      <td>small</td>\n",
       "      <td>med</td>\n",
       "      <td>acc</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1721</th>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>5-more</td>\n",
       "      <td>more</td>\n",
       "      <td>small</td>\n",
       "      <td>high</td>\n",
       "      <td>good</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1722</th>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>5-more</td>\n",
       "      <td>more</td>\n",
       "      <td>med</td>\n",
       "      <td>low</td>\n",
       "      <td>unacc</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1723</th>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>5-more</td>\n",
       "      <td>more</td>\n",
       "      <td>med</td>\n",
       "      <td>med</td>\n",
       "      <td>good</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1724</th>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>5-more</td>\n",
       "      <td>more</td>\n",
       "      <td>med</td>\n",
       "      <td>high</td>\n",
       "      <td>vgood</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1725</th>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>5-more</td>\n",
       "      <td>more</td>\n",
       "      <td>big</td>\n",
       "      <td>low</td>\n",
       "      <td>unacc</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1726</th>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>5-more</td>\n",
       "      <td>more</td>\n",
       "      <td>big</td>\n",
       "      <td>med</td>\n",
       "      <td>good</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1727</th>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>5-more</td>\n",
       "      <td>more</td>\n",
       "      <td>big</td>\n",
       "      <td>high</td>\n",
       "      <td>vgood</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        1    2       3     4      5     6      7  cluster\n",
       "1718  low  low  5-more   4.0    big  high  vgood        2\n",
       "1719  low  low  5-more  more  small   low  unacc        2\n",
       "1720  low  low  5-more  more  small   med    acc        2\n",
       "1721  low  low  5-more  more  small  high   good        2\n",
       "1722  low  low  5-more  more    med   low  unacc        2\n",
       "1723  low  low  5-more  more    med   med   good        2\n",
       "1724  low  low  5-more  more    med  high  vgood        2\n",
       "1725  low  low  5-more  more    big   low  unacc        2\n",
       "1726  low  low  5-more  more    big   med   good        2\n",
       "1727  low  low  5-more  more    big  high  vgood        2"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['cluster'] = test.get_labels()\n",
    "\n",
    "# Display last 10 entries\n",
    "data.tail(10)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
